---
title: 'deep learning 入门资料和环境配置'
date: 2019-03-12
permalink: /posts/2019/03/blog-post-2/
tags:
  - deep learning
  - introduction
---

## 论文报告2018-2019学年上学期
@[沈星, 21635035@zju.edu.cn, Tel:13276711080]

[TOC]

### Bayesian deep convolutional encoder–decoder networks for surrogate modeling and uncertainty quantification
- 作者 YinhaoZhu, NicholasZabaras
- 来源 Journal ofComputationalPhysics366(2018)415–447
- 时间 2018
#### 论文亮点
BNN 把神经网络的参数看做是随机变量，而不是定值，为了解决数据不足导致的认知不确定性。
模型最后的输出不仅仅是方程的解的值，还有方程的解的分布与方差
BNN模型可以用很少的数据量训练一个很复杂的模型。是其他的深度学习模型做不到的。
BNN本身就具有防止过拟合的效果。有很强的鲁棒性
#### 论文使用到的方法
BNN将网络中的每一个参数都看做是一个随机变量，并且在训练过程中不断调整参数的分布。
BNN的更新要使用变分推断进行
##### 变分推断
training inputs:$\mathbf { X } = \left\{ \mathbf { x } _ { 1 } , \ldots , \mathbf { x } _ { N } \right\}$
corresponding outputs$\mathbf { Y } = \left\{ \mathbf { y } _ { 1 } , \ldots , \mathbf { y } _ { N } \right\}$
找到参数w使得$\mathbf { y } = \mathbf { f } ^ { \omega } ( \mathbf { x } )$
先验信息:$p(\omega)$
分类问题下softmax 后的概率
$p ( y = d | \mathrm { x } , \omega ) = \frac { \exp \left( f _ { d } ^ { \omega } ( \mathrm { x } ) \right) } { \sum _ { d ^ { \prime } } \exp \left( f _ { d ^ { \prime } } ^ { \omega } ( \mathrm { x } ) \right) }$
或者Gaussian likelihood for regression
$p ( \mathbf { y } | \mathbf { x } , \boldsymbol { \omega } ) = \mathcal { N } \left( \mathbf { y } ; \mathbf { f } ^ { \omega } ( \mathbf { x } ) , \tau ^ { - 1 } I \right)$
with model precision $τ$ . This can be seen as corrupting the model output with observation noise with variance $τ^{−1}$.
使用贝叶斯公式
$p ( \boldsymbol { \omega } | \mathbf { X } , \mathbf { Y } ) = \frac { p ( \mathbf { Y } | \mathbf { X } , \boldsymbol { \omega } ) p ( \boldsymbol { \omega } ) } { p ( \mathbf { Y } | \mathbf { X } ) }$
$p ( \mathbf { Y } | \mathbf { X } , \boldsymbol { \omega })$可以算出， $p ( \boldsymbol { \omega } )$是先验分布。

Inference process:
给定一个新的输入点$x^*$
$p \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathbf { X } , \mathbf { Y } \right) = \int p \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \boldsymbol { \omega } \right) p ( \boldsymbol { \omega } | \mathbf { X } , \mathbf { Y } ) \mathrm { d } \omega$
normaliser or model evidence:
$p ( \mathbf { Y } | \mathbf { X } ) = \int p ( \mathbf { Y } | \mathbf { X } , \boldsymbol { \omega } ) p ( \boldsymbol { \omega } ) \mathrm { d } \boldsymbol { \omega }$
marginal likelihood
大多数情况下$ p ( \mathbf { Y } | \mathbf { X } ) $是没有解析解的，所以要用到变分推断
找到一个简单的分布$q _ { \theta } ( \boldsymbol { \omega } )$与所要求的分布尽量的接近。
$\mathrm { KL } \left( q _ { \theta } ( \omega ) \| p ( \omega | \mathbf { X } , \mathbf { Y } ) \right) = \int q _ { \theta } ( \omega ) \log \frac { q _ { \theta } ( \boldsymbol { \omega } ) } { p ( \boldsymbol { \omega } | \mathbf { X } , \mathbf { Y } ) } \mathrm { d } \boldsymbol { \omega }$
这里定义$q _ { \theta } ( \boldsymbol { \omega } )$ 是关于$p ( \boldsymbol { \omega } | \mathbf { X } , \mathbf { Y } )$完全连续的.
对于事件A，$p ( A | \mathbf { X } , \mathbf { Y } ) = 0$ implies $q _ { \theta } ( A ) = 0 $
设$q _ { \theta } ^ { * } ( \omega )$是目标函数的最优解。
这样就可以用$q _ { \theta } ^ { * } ( \omega )$代替$p ( \boldsymbol { \omega } | \mathbf { X } , \mathbf { Y } )$
$p \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathbf { X } , \mathbf { Y } \right) \approx \int p \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \omega \right) q _ { \theta } ^ { * } ( \boldsymbol { \omega } ) \mathrm { d } \omega = : q _ { \theta } ^ { * } \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } \right)$
问题可以通过计算转换为:
$K L ( Q \| P ) = \int Q ( Z ) \log \frac { Q ( Z ) } { P ( Z | X ) } d Z$
$= - \int Q ( Z ) \log \frac { P ( Z | X ) } { Q ( Z ) } d Z$
$= - \int Q ( Z ) \log \frac { P ( Z , X ) } { Q ( Z ) P ( X ) } d Z$
$= \int Q ( Z ) [ \log Q ( Z ) + \log P ( X ) ] d Z - \int Q ( Z ) \log P ( Z , X ) d Z$
$=\int Q(Z) \log P ( X )dZ + \int Q ( Z ) \log Q ( Z ) d Z - \int Q ( Z ) \log P ( Z , X ) d Z$
所以问题换位最大化evidence lower bound (ELBO):
$\mathcal { L } _ { \mathrm { VI } } ( \theta ) : = \int q _ { \theta } ( \boldsymbol { \omega } ) \log p ( \mathbf { Y } | \mathbf { X } , \boldsymbol { \omega } ) \mathrm { d } \boldsymbol { \omega } - \mathrm { KL } \left( q _ { \theta } ( \boldsymbol { \omega } ) \| p ( \boldsymbol { \omega } ) \right) \leq \log p ( \mathbf { Y } | \mathbf { X } ) = \log$ evidence
最大化第一项(expected log lokelihood) 即为让$q _ { \theta } ( \boldsymbol { \omega } )$更好的解释数据
最小化第二项(prior KL)即为让$q _ { \theta } ( \boldsymbol { \omega } )$跟先验分布更接近。
This acts as an “Occam razor” term and penalises complex distributions$q _ { \theta } ( \boldsymbol { \omega } )$
保持了贝叶斯模型的好处，即拟合了数据，又用最简单的方法来
变分推断把积分计算变成了优化过程(求导)。
与深度学习的优化过程相比，这里优化的是一个分布，而不是点估计。
Note that optimisation in the deep learning sense can be recovered by setting the approximating distribution as a delta
##### SVGD
SVGD是在变分推断的基础上，与SGD结合产生的算法，在更新BNN上效果很好。与其他的算法相比有
1在数据量大时表现很好，2很容易用于复杂的模型中，且不用改变模型原本的架构。3使用起来很方便，不需要知道原理也可以使用。
![Alt text](./1547699242244.png)

#### 论文主要内容
方程:
$\mathbf { u } ( \mathbf { s } ) = - K ( \mathbf { s } ) \nabla p ( \mathbf { s } ) , \quad \mathbf { s } \in \mathcal { S }$
$\nabla \cdot \mathbf { u } ( \mathbf { s } ) = f ( \mathbf { s } ) , \quad \mathbf { s } \in \mathcal { S }$
$\mathbf { u } ( \mathbf { s } ) \cdot \hat { \mathbf { n } } ( \mathbf { s } ) = 0 , \quad \mathbf { s } \in \partial \mathcal { S }$
$\int _ { \mathcal { S } } p ( \mathbf { s } ) d \mathbf { s } = 0$
其中:
$\mathcal { S } = [ 0,1 ] ^ { 2 }$
$f ( \mathbf { s } ) = \left\{ \begin{array} { l l } { r , } & { \text { if } \left| \mathbf { s } _ { i } - \frac { 1 } { 2 } w \right| \leq \frac { 1 } { 2 } w , \text { for } i = 1,2 } \\ { - r , } & { \text { if } \left| \mathbf { s } _ { i } - 1 + \frac { 1 } { 2 } w \right| \leq \frac { 1 } { 2 } w , \text { for } i = 1,2 } \\ { 0 , } & { \text { otherwise } } \end{array} \right.$
$K ( \mathbf { s } ) = \exp ( G ( \mathbf { s } ) ) , \quad G ( \cdot ) \sim \mathcal { N } ( m , k ( \cdot , \cdot ) )$
$k \left( \mathbf { s } , \mathbf { s } ^ { \prime } \right) = \exp \left( - \left\| \mathbf { s } - \mathbf { s } ^ { \prime } \right\| _ { 2 } / l \right)$
将空间进行网格化拆分为65*65的网格:
$\mathcal { D } = \left\{ \mathrm { x } ^ { i } , \mathrm { y } ^ { i } \right\} _ { i = 1 } ^ { N } ,$ where $\mathrm { x } ^ { i } \in \mathbb { R } ^ { d _ { x } \times H \times W }$
$\mathbf { y } ^ { i } \in \mathbb { R } ^ { d _ { y } \times H \times W }$
输入是u(65*65),输出是$u_x,u_y,p$
![Alt text](./1547710588123.png)
![Alt text](./1547711608185.png)
训练数据:
用了256个训练数据。
训练数据用FEniCS生成，velocity用third-order Raviart–Thomas elements，p使用fourth-order discontinuous elements.

##### inference
Predictive uncertainty at $\mathbf { x } ^ { * }$
$p \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathcal { D } \right) = \int p \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathbf { w } , \beta \right) p ( \mathbf { w } , \beta | \mathcal { D } ) d \mathbf { w } d \beta$
积分部分使用Monte Carlo计算
$\begin{aligned} \mathbb { E } \left[ \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathcal { D } \right] & = \mathbb { E } _ { p ( \mathbf { w } , \beta | \mathcal { D } ) } \left[ \mathbb { E } \left[ \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathbf { w } , \beta \right] \right] \\ & = \mathbb { E } _ { p ( \mathbf { w } | \mathcal { D } ) } \left[ \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } \right) \right] \\ & \approx \frac { 1 } { S } \sum _ { i = 1 } ^ { S } \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } ^ { i } \right) , \quad \mathbf { w } ^ { i } \sim p ( \mathbf { w } | \mathcal { D } ) \end{aligned}$

$\begin{aligned} \operatorname { Cov } \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathcal { D } \right) & = \mathbb { E } _ { \mathbf { w } , \beta } \left[ \operatorname { Cov } \left( \mathbf { y } ^ { * } | \mathbf { w } , \beta , \mathbf { x } ^ { * } \right) \right] + \operatorname { Cov } _ { \mathbf { w } , \beta } \left( \mathbb { E } \left[ \mathbf { y } ^ { * } | \mathbf { w } , \beta , \mathbf { x } ^ { * } \right] \right) \\ & = \mathbb { E } _ { \mathbf { w } , \beta } \left[ \beta ^ { - 1 } \mathbf { I } \right] + \operatorname { Cov } _ { \mathbf { w } , \beta } \left( \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } \right) \right) \\ & = \mathbb { E } _ { \beta } \left[ \beta ^ { - 1 } \mathbf { I } \right] + \mathbb { E } _ { \mathbf { w } } \left[ \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } \right) \mathbf { f } ^ { \top } \left( \mathbf { x } ^ { * } , \mathbf { w } \right) \right] - \mathbb { E } _ { \mathbf { w } } \left[ \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } \right) \right] \mathbb { E } _ { \mathbf { w } } ^ { \top } \left[ \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } \right) \right] \end{aligned}$
$\begin{aligned} \approx & \frac { 1 } { S } \sum _ { i = 1 } ^ { S } \left( \left( \beta ^ { i } \right) ^ { - 1 } \mathbf { I } + \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } ^ { i } \right) \mathbf { f } ^ { \top } \left( \mathbf { x } ^ { * } , \mathbf { w } ^ { i } \right) \right) \\ & - \left( \frac { 1 } { S } \sum _ { i = 1 } ^ { S } \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } ^ { i } \right) \right) \left( \frac { 1 } { S } \sum _ { i = 1 } ^ { S } \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } ^ { i } \right) \right) ^ { \top } \end{aligned}$
$\begin{aligned} \operatorname { Var } \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathcal { D } \right) & = \operatorname { diag } \operatorname { Cov } \left( \mathbf { y } ^ { * } | \mathbf { x } ^ { * } , \mathcal { D } \right) \\ & = \frac { 1 } { S } \sum _ { i = 1 } ^ { S } \left( \left( \beta ^ { i } \right) ^ { - 1 } \mathbf { 1 } + \mathbf { f } ^ { 2 } \left( \mathbf { x } ^ { * } , \mathbf { w } ^ { i } \right) \right) - \left( \frac { 1 } { S } \sum _ { i = 1 } ^ { S } \mathbf { f } \left( \mathbf { x } ^ { * } , \mathbf { w } ^ { i } \right) \right) ^ { 2 } \end{aligned}$
所以这样计算出了输出的分布，均值和方差

##### 实验结果:
![Alt text](./1547711816128.png)
用了不多的数据就可以计算出较好的结果。

### Generative Adversarial Nets
- 作者 Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley,
Sherjil Ozairy, Aaron Courville, Yoshua Bengioz
- 来源 [arXiv:1406.2661](https://arxiv.org/abs/1406.2661)
- 时间 2014
#### 论文主要内容
GAN使用对抗式训练的方法使得可以使用极少数据甚至不需要标注数据训练很大的网络。在各个领域都有很大的应用。
GAN由Generator和Discriminator组成，由G生成样本，然后D进行判断样本是来自于真实样本还是G生成的。
$\min _ { G } \max _ { D } V ( D , G ) = \mathbb { E } _ { \boldsymbol { x } \sim p _ {$ data $} ( \boldsymbol { x } ) } [ \log D ( \boldsymbol { x } ) ] + \mathbb { E } _ { \boldsymbol { z } \sim p _ { \boldsymbol { z } } ( \boldsymbol { z } ) } [ \log ( 1 - D ( G ( \boldsymbol { z } ) ) ) ]$
训练时先优化D,max第一项即为使D能更好地判断出真实样本，max第二项即为使D能判断出来自于G生成的样本。
接着优化G,min第二项即为使G生成的样本能混淆D的判断。使生成的样本更真实。
当训练收敛时，训练好的D就是一个很好的判别模型，能够分辨出数据是否来自真实样本。G可以生成难以分辨真假样本。
![Alt text](./1547714672791.png)
##### 缺点:
必须小心平衡生成器和判别器的训练程度，如果判别器训练的太好，生成器梯度消失，判别器训练得不好，生成器梯度方差很大。
生成器和判别器的loss无法指示训练进程
生成样本缺乏多样性

#### Towards Principled Methods for Training Generative Adversarial Networks
这篇文章主要从理论上分析了原始的GAN出现的问题和原因，提出了解决方法。
![Alt text](./1547714955664.png)
当$P_r$与$P_g$的支集是高维空间中的低维流形时，$P_r$与$P_g$重叠部分测度为0的概率为1。
![Alt text](./1547714962281.png)
在近似最优判别器下，最小化生成器的loss等价于最小化P_r与P_g之间的JS散度，而由于P_r与P_g几乎不可能有不可忽略的重叠，所以无论它们相距多远JS散度都是常数，最终导致生成器的梯度趋于0，梯度消失。
![|center|400x0](./1547714975491.png)
![Alt text](./1547714982144.png)
判别器训练得太好，生成器梯度消失，生成器loss降不下去；判别器训练得不好，生成器梯度不准，四处乱跑。
![|center|400x0](./1547714996602.png)

collapse mode:
$\mathbb { E } _ { x \sim P _ { r } } \left[ \log D ^ { * } ( x ) \right] + \mathbb { E } _ { x \sim P _ { g } } \left[ \log \left( 1 - D ^ { * } ( x ) \right) \right] = 2 J S \left( P _ { r } \| P _ { g } \right) - 2 \log 2$
$\begin{aligned} K L \left( P _ { g } \| P _ { r } \right) & = \mathbb { E } _ { x \sim P _ { g } } \left[ \log \frac { P _ { g } ( x ) } { P _ { r } ( x ) } \right] \\ & = \mathbb { E } _ { x \sim P _ { g } } \left[ \log \frac { P _ { g } ( x ) / \left( P _ { r } ( x ) + P _ { g } ( x ) \right) } { P _ { r } ( x ) / \left( P _ { r } ( x ) + P _ { g } ( x ) \right) } \right] \\ & = \mathbb { E } _ { x \sim P _ { g } } \left[ \log \frac { 1 - D ^ { * } ( x ) } { D ^ { * } ( x ) } \right] \\ & = \mathbb { E } _ { x \sim P _ { g } } \log \left[ 1 - D ^ { * } ( x ) \right] - \mathbb { E } _ { x \sim P _ { g } } \log D ^ { * } ( x ) \end{aligned}$
$\begin{aligned} \mathbb { E } _ { x \sim P _ { g } } \left[ - \log D ^ { * } ( x ) \right] & = K L \left( P _ { g } \| P _ { r } \right) - \mathbb { E } _ { x \sim P _ { g } } \log \left[ 1 - D ^ { * } ( x ) \right] \\ & = K L \left( P _ { g } \| P _ { r } \right) - 2 J S \left( P _ { r } \| P _ { g } \right) + 2 \log 2 + \mathbb { E } _ { x \sim P _ { r } } \left[ \log D ^ { * } ( x ) \right] \end{aligned}$
原方程变为:
$K L \left( P _ { g } \| P _ { r } \right) - 2 J S \left( P _ { r } \| P _ { g } \right)$
- 当$P_g(x)->0$,$P_r(x)->1$时，即生成器没能生成真实的样本，$P_g(x)log\frac{P_g(x)}{{P_r(x)}}->0$
- 当$P_g(x)->1$,$P_r(x)->0$时，即生成器生成了不真实的样本，$P_g(x)log\frac{P_g(x)}{{P_r(x)}}->\inf $
所以生成器会生成很多重复的样本，而不是生成多样性的样本。


----------


#### Wasserstein GAN
文章使用了Wasserstein 距离。
$W \left( P _ { r } , P _ { g } \right) = \inf _ { \gamma \sim \Pi \left( P _ { r } , P _ { g } \right) } \mathbb { E } _ { ( x , y ) \sim \gamma } [ \| x - y \| ]$
所以，使用了Wasserstein距离后，$P_r \cap  P_g$的测度即使为0，仍然可以仍然可以反映距离的远近
![|center|400x200](./1547715217685.png)
$K L \left( P _ { 1 } \| P _ { 2 } \right) = K L \left( P _ { 1 } \| P _ { 2 } \right) = \left\{ \begin{array} { l l } { + \infty } & { \text { if } \theta \neq 0 } \\ { 0 } & { \text { if } \theta = 0 } \end{array} \right.$
$W \left( P _ { 0 } , P _ { 1 } \right) = | \theta |$
根据Kantorovich-Rubinstein duality:
$W \left( P _ { r } , P _ { g } \right) = \frac { 1 } { K } \sup _ { \| f \| _ { L } \leq K } \mathbb { E } _ { x \sim P _ { r } } [ f ( x ) ] - \mathbb { E } _ { x \sim P _ { g } } [ f ( x ) ]$
在满足Lipschitz条件下:
$\left| f \left( x _ { 1 } \right) - f \left( x _ { 2 } \right) \right| \leq K \left| x _ { 1 } - x _ { 2 } \right|$
因为$f(x) = R(W(R(W(x))))$...N层神经网络，所以当$||W||<C$时，$|f(x1)-f(x2)|<C^N|x_1-x_2|$
于是用参数截断(weight clipping)代替Lipschitz条件.
此时的损失定义为:
$$Loss_G = -E_{x \sim P_g}[f_w(x)]$$
$$Loss_D = E_{x \sim P_g}[f_w(x)]- E_{x \sim P_r}[f_w(x)]$$
所以最终算法为:
![Alt text](./1547715677758.png)

##### WGAN与Origin GAN的区别:
- 判别器最后一层去掉sigmoid
- 生成器和判别器的loss不取log
- 每次更新判别器的参数之后把它们的绝对值截断到不超过一个固定常数c
- 不要用基于动量的优化算法，推荐RMSProp，SGD。(这里是根据经验得出)
#### Improved Training of Wasserstein GANs
weight clipping 会导致参数的值集中在c 和-c两点。同时可能会导致梯度爆炸或者梯度消失
![|center|400x0](./1547715753044.png)
![|center|400x0](./1547715758747.png)
直接使用lips条件作为损失
$\hat { x } = \epsilon x _ { r } + ( 1 - \epsilon ) x _ { g }$
$L ( D ) = - \mathbb { E } _ { x \sim P _ { r } } [ D ( x ) ] + \mathbb { E } _ { x \sim P _ { g } } [ D ( x ) ] + \lambda \mathbb { E } _ { x \sim P _ { \dot { x } } } \left[ \left\| \nabla _ { x } D ( x ) \right\| _ { p } - 1 \right] ^ { 2 }$
(无法再整个空间加上lips条件，所以在分布r和分布g直接加上lips条件)
使用了以上的方法进行训练后，能显著降低训练GAN的难度。并且提高效果。
#### GAN的应用
##### DCGAN
[Machine Learning Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks](https://arxiv.org/pdf/1511.06434.pdf)
训练数据
![|center|400x0](./1547717276172.png)
![|center|0x400](./1547717436208.png)
G(z)把图像从z(100维)变为3*64*64
D(x) 把x(3*64*64)变为1维(图片是否是真实图片的概率)
损失:
$\begin{align}\ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad l_n = - \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right]\end{align}$
训练过程:
(1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))
先用真实数据训练:(image,label)，label =1
Loss = BCELoss(D(image),label)
再用假的数据训练:
从正态分布中获取100维的noise向量z
G(z)即为生成的假的图片,label=0
Loss = BCELoss(D(G(z)))
(2) Update G network: maximize log(D(G(z)))
label = 1
Loss = BCELoss(D(G(z),label))
GAN在图片生成上的效果
![|center|500*0](./1547717495553.png)

#### GAN在微分方程正问题上的应用
##### Deep learning the Phtsics of Transport Phenomena
作者: Amir Barati Farimani,⇤ Joseph Gomes,⇤ and Vijay S. Pande
来源:arxiv
时间:2017
问题:
热方程的正问题
$$\frac { \partial ^ { 2 } T } { \partial x ^ { 2 } } + \frac { \partial ^ { 2 } T } { \partial y ^ { 2 } } = 0, \space x,y \in \Omega$$
$$T(x,y) = f(x,y),\space x,y \in \partial \Omega$$
输入为初始和边界条件，
比如:
$\mathrm { T } ( \mathrm { x } , 0 ) = 60 ^ { \circ } \mathrm { C } ; \mathrm { T } ( \mathrm { x } , 54 ) = 10 ^ { \circ } \mathrm { C } ; \mathrm { T } ( 0 , \mathrm { y } ) = 70 ^ { \circ } \mathrm { C } ; \mathrm { T } ( 39 , \mathrm { y } ) = 100 ^ { \circ } \mathrm { C }$
对$\Omega$进行网格剖分为64*64。
G:输入为64*64*1，输出为64*64*1.输出为方程的解。
模型为:
![Alt text](./1548033295747.png)
采用了Unet的网络架构。参数量相对较少。
D:输入为64*64*2输出是定值，表示是否来自同一个分布的概率。D判断unknown的64*64*1的分布是否来自于input的64*64*1
模型为:
![Alt text](./1548035947509.png)

模型效果：
用6000个训练数据训练，可以达到效果MAE<1%的准确率。
![Alt text](./1548036254768.png)